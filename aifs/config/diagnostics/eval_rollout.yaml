eval:
  enabled: False
  # use this to evaluate the model over longer rollouts, every so many validation batches
  rollout: 12
  frequency: 20
plot:
  enabled: True
  frequency: 750
  sample_idx: 0
  per_sample: 6
  parameters:
    # - t_137
    # - u_137
    # - v_137
    - 2t
    - 10u
    - 10v
    - sp
    - tp
    - cp
  learned_features: True

debug:
  # this will detect and trace back NaNs / Infs etc. but will slow down training
  anomaly_detection: False

# activate the pytorch profiler (disable this in production)
# remember to also activate the tensorboard logger (below)
profiler: False

checkpoint:
  save_frequency:
    every_n_minutes: 30 # Approximate, as this is checked at the end of training steps
    every_n_epochs: 1
    every_n_train_steps: null # Does not scale with rollout
  # If set to k, saves the best k model weights in the training. 
  # If set to -1, all checkpoints are kept ensuring runs can be continued/forked at any point in the training process
  num_models_saved: -1

log:
  wandb:
    enabled: True
    offline: False
    log_model: False
    # logger options (these probably come with some overhead)
    gradients: False
    parameters: False
  tensorboard:
    enabled: False
  interval: 100
