# resume or fork a training from a checkpoint last.ckpt or specified in hardware.files.warm_start
run_id: null
fork_run_id: null

# prescribe initial seed, if null generate seed
initial_seed: null

# run in deterministic mode ; slows down
deterministic: False

# miscellaneous
precision: 16-mixed

# multistep input
# 1 = single step scheme, X(t-1) used to predict X(t)
# k > 1: multistep scheme, uses [X(t-k), X(t-k+1), ... X(t-1)] to predict X(t)
# Deepmind use k = 2 in their model
multistep_input: 2

# gradient accumulation across K batches, K >= 1 (if K == 1 then no accumulation)
# the effective batch size becomes num-devices * batch_size * k
accum_grad_batches: 1

# clipp gradients, 0 : don't clip, default algorithm: norm, alternative: value
gradient_clip:
  val: 32.
  algorithm: value

# stochastic weight averaging
# https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/
swa:
  enabled: False
  lr: 1.e-4

# use ZeroRedundancyOptimizer ; saves memory for larger models
zero_opt: False

# dynamic rescaling of the loss gradient
# see https://arxiv.org/pdf/2306.06079.pdf, section 4.3.2
# don't enable this by default until it's been tested and proven beneficial
loss_gradient_scaling: False

# length of the "rollout" window (see Keisler's paper)
rollout:
  start: 1
  # increase rollout every n epochs
  epoch_increment: 0
  # maximum rollout to use
  max: 1

# Keisler's three training rounds were:
# Round 1. ~960,000 batches @ ~0.3 seconds per batch (4-step rollout)
# Round 2. ~90,000 batches @ ~1.0 seconds per batch (8-step rollout)
# Round 3. ~70,000 batches @ ~1.5 seconds per batch (12-step rollout)
# Each batch is an N-step rollout, with batch_size=1

save_top_k: -1

lead_time: 6
max_epochs: 200
lr:
  rate: 0.625e-4
  iterations: 600000
  min: 1.5e-7 #Not scaled by #GPU

loss_scaling:
  pl:
    q: 0.6 #1
    t: 6 #1
    u: 0.8 #0.5
    v: 0.5 #0.33
    w: 0.001
    z: 12 #1
  sfc:
    sp: 0.1
    sst: 0.1
    10u: 0.1
    10v: 0.1
    2d: 0.1

metrics:
  - z_500
  - t_850
  - u_850
  - v_850
  - 2t
  - msl
