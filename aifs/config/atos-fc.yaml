input:
  # zarr is also supported
  # the zarr data are under /ec/res4/scratch/syma/era5/o160/zarr/[pl | sfc]
  format: zarr
  resolution: o96
  training: 
    basedir: /lus/h2resw01/fws4/lb/project/ai-ml/zarrs/
    filename: aifs-era5-{resolution}-1979-2015-1h.zarr
  validation:
    basedir: /lus/h2resw01/fws4/lb/project/ai-ml/zarrs/
    filename: aifs-era5-{resolution}-2016-2017-1h.zarr
  test:
    basedir: /lus/h2resw01/fws4/lb/project/ai-ml/zarrs/
    filename: aifs-era5-{resolution}-2016-2017-1h.zarr
  predict:
    basedir: /lus/h2resw01/fws4/lb/project/ai-ml/zarrs/
    filename: aifs-era5-{resolution}-2016-2017-1h.zarr
  pl:
      names:
      # not used!
        - q
        - t
        - u
        - v
        - w
        - z
      levels:
        - 50
        - 100
        - 150
        - 200
        - 250
        - 300
        - 400
        - 500
        - 600
        - 700
        - 850
        - 925
        - 1000
  sfc:
      names:
      # not used!
        - msl
        - z
        - lsm
  # features that are not part of the forecast state
  # these are presumed to be located at the "end" of the flattened (bs, latlon, nvar) input tensor
  num-features: 98
  num-aux-features: 13  # all 2D fields
  loss-scaling-pl:
    - 0.6
    - 6
    - 0.8
    - 0.5
    - 0.01
    - 12
  loss-scaling-sfc:
    - 0.1
    - 1
    - 0.1
    - 0.1
    - 0.1
    - 1
    - 0.1

metrics:
  - z_500
  - t_850
  - u_850
  - v_850
  - 2t
  - msl

plot:
#not yet used
  - q_850
  - t_850
  - u_850
  - v_850
  - w_850 
  - z_850
  - msl
  - 2t

diagnostics:
  eval:
    enabled: True
    # use this to evaluate the model over longer rollouts, every so many validation batches
    rollout: 12
    frequency: 75

###################
#  OUTPUT BLOCK
###################
output:
  basedir: /ec/res4/scratch/syma/GNN/ERA5/{resolution}/
  logging:
    log-dir: logs
    log-interval: 500
  checkpoints:
    # model checkpoints: used to resume training or do inference
    ckpt-dir: checkpoints
  model:
    save-top-k: 10
    checkpoint-filename: aifs-fc-ckpt-{epoch:02d}-{val_wmse:.4f}
  plots:
    plot-dir: plots

###################
#  GRAPH BLOCK
###################
graph:
  data-basedir: /ec/res4/hpcperm/nesl/gnn/
  data-file: graph_mappings_normed_edge_attrs1_fixed_weights_{resolution}_h_0_1_2_3_4_5.pt

###################
#  MODEL BLOCK
###################
model:
  # torch.compile is not working (yet), set this option to False 
  compile: False
  debug:
    # this will detect and trace back NaNs / Infs etc. but will slow down training
    anomaly-detection: False
  wandb:
    enabled: True
  dataloader:
    num-workers:
      training: 12
      validation: 12
      test: 8
      predict: 8
    batch-size:
      training: 4
      validation: 4
      test: 4
      predict: 4

  # logger options (these probably come with some overhead)
  log-gradients: False
  log-parameters: False

  ae:
    # full path
    ckpt-path: /ec/res4/scratch/syma/GNN-AE/ERA5/o96/checkpoints/20230601_1757/aifs-ae-1h-ckpt-epoch=17-val_ae_wmse=0.0000.ckpt
    # /ec/res4/scratch/syma/GNN-AE/ERA5/o96/checkpoints/20230531_1618/aifs-ae-ckpt-epoch=95-val_ae_wmse=0.0000.ckpt
    weights-prefix:
      encoder: ae.encoder
      decoder: ae.decoder

  # resume training from a checkpoint
  warm-restart:
    enabled: False
    # path should be relative to the output:checkpoints:chkpt-dir, as defined above
    ckpt-path: 202304_1014/aifs-resolution=0-ckpt-epoch=49-val_wmse=0.010.ckpt
  
  # miscellaneous
  precision: 16-mixed  # "16" with Lightning < 2.0

  # runs only N training batches [N = integer | null]
  # if null then we run through all the batches
  limit-batches:
    training: null
    validation: null
    test: 20
    predict: 20

  # parallel strategy
  # if running on a single device set to "null" [Lightning < 2] or "auto" [Lightning 2.0]
  strategy: auto
  # number of GPUs per node and number of nodes (for DDP)
  num-gpus: 4
  num-nodes: 1

  # training settings
  lead-time: 6
  max-epochs: 100
  learn-rate: 0.000125

  # multistep input
  # 1 = single step scheme, X(t-1) used to predict X(t)
  # k > 1: multistep scheme, uses [X(t-k), X(t-k+1), ... X(t-1)] to predict X(t)
  # Deepmind use k = 2 in their model
  multistep-input: 1

  # # gradient accumulation across K batches, K >= 1 (if K == 1 then no accumulation)
  # # the effective batch size becomes num-devices * batch-size * k
  # accum-grad-batches: 1

  # # clipp gradients, 0 : don't clip, default algorithm: norm, alternative: value
  # gradient-clip-val: 32.
  # gradient-clip-algorithm: value

  # stochastic weight averaging
  # https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/
  swa:
    enabled: False
    lr: 1.e-4

  # activate the pytorch profiler (disable this in production)
  profile: False

  # specific GNN encoder settings
  encoder:
    mapper-num-layers: 2
    num-layers: 16
    activation: SiLU
    num-out-channels: 512
    mlp-extra-layers: 0
    dropout: 0.

  # length of the "rollout" window (see Keisler's paper)
  rollout: 1
