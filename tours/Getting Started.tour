{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "[AIFS] Getting Started",
  "steps": [
    {
      "file": "aifs/train/train.py",
      "description": "For AIFS training, we are using Hydra configs to make settings highly configurable.\n\nThis means that you can run aifs training in default mode:\n\n>> aifs-train\n\nYou can switch out the config to a different main setting:\n\n>> aifs-train --config-name=debug\n\nAnd you can even change settings on-the-fly in your command line!\n\n>> aifs-train training.max_epochs=5\n\n>> aifs-train --config-name=debug training.max_epochs=5\n\nThis then simply calls the `train()` method of our `AIFSTrainer` class.",
      "line": 197,
      "title": "How to Train a GNN",
      "contents": "from functools import cached_property\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport hydra\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.profilers import PyTorchProfiler\n\nfrom aifs.data.era_datamodule import ERA5DataModule\nfrom aifs.diagnostics.callbacks import get_callbacks\nfrom aifs.diagnostics.logging import get_tensorboard_logger\nfrom aifs.diagnostics.logging import get_wandb_logger\nfrom aifs.distributed.strategy import DDPGroupStrategy\nfrom aifs.train.forecaster import GraphForecaster\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass AIFSTrainer:\n    \"\"\"Utility class for training the model.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        # Set the default precision for all matmul operations to float32\n        torch.set_float32_matmul_precision(\"high\")\n        # Resolve the config to avoid shenanigans with lazy loading\n        OmegaConf.resolve(config)\n        self.config = config\n\n        # Default to not warm-starting from a checkpoint\n        self.start_from_checkpoint = bool(self.config.training.run_id) or bool(self.config.training.fork_run_id)\n        self.config.training.run_id = self.run_id\n\n        # Update paths to contain the run ID\n        self.update_paths()\n\n        self.log_information()\n\n    @cached_property\n    def datamodule(self) -> ERA5DataModule:\n        \"\"\"DataModule instance and DataSets.\"\"\"\n        return ERA5DataModule(self.config)\n\n    @cached_property\n    def initial_seed(self) -> int:\n        \"\"\"Initial seed for the RNG.\"\"\"\n        initial_seed = pl.seed_everything(self.config.training.initial_seed, workers=True)\n        LOGGER.debug(\"Running with initial seed: %d\", initial_seed)\n        return initial_seed\n\n    @cached_property\n    def model(self) -> GraphForecaster:\n        \"\"\"Provide the model instance.\"\"\"\n        return GraphForecaster(metadata=self.datamodule.input_metadata, config=self.config)\n\n    @cached_property\n    def run_id(self) -> str:\n        \"\"\"Unique identifier for the current run.\"\"\"\n\n        if self.config.training.run_id and not self.config.training.fork_run_id:\n            # Return the provided run ID\n            return self.config.training.run_id\n\n        # Generate a random UUID\n        import uuid\n\n        return str(uuid.uuid4())\n\n    @cached_property\n    def wandb_logger(self) -> pl.loggers.WandbLogger:\n        \"\"\"WandB logger.\"\"\"\n        return get_wandb_logger(self.config, self.model)\n\n    @cached_property\n    def tensorboard_logger(self) -> pl.loggers.TensorBoardLogger:\n        \"\"\"TensorBoard logger.\"\"\"\n        return get_tensorboard_logger(self.config)\n\n    @cached_property\n    def last_checkpoint(self) -> Optional[str]:\n        \"\"\"Path to the last checkpoint.\"\"\"\n        if not self.start_from_checkpoint:\n            return None\n\n        checkpoint = Path(\n            self.config.hardware.paths.checkpoints.parent,\n            self.config.training.fork_run_id or self.run_id,\n            self.config.hardware.files.warm_start or \"last.ckpt\",\n        )\n\n        # Check if the last checkpoint exists\n        if Path(checkpoint).exists():\n            LOGGER.info(\"Resuming training from last checkpoint: %s\", checkpoint)\n            return checkpoint\n        LOGGER.warning(\"Could not find last checkpoint: %s\", checkpoint)\n\n    @cached_property\n    def callbacks(self) -> List[pl.callbacks.Callback]:\n        return get_callbacks(self.config)\n\n    @cached_property\n    def profiler(self) -> Optional[PyTorchProfiler]:\n        \"\"\"Returns a pytorch profiler object, if profiling is enabled, otherwise\n        None.\"\"\"\n        if self.config.diagnostics.profiler:\n            assert (\n                self.config.diagnostics.log.tensorboard.enabled\n            ), \"Tensorboard logging must be enabled when profiling! Check your job config.\"\n            return PyTorchProfiler(\n                dirpath=self.config.hardware.paths.logs.tensorboard,\n                filename=\"aifs-profiler\",\n                export_to_chrome=False,\n                # profiler-specific keywords\n                activities=[\n                    # torch.profiler.ProfilerActivity.CPU,  # this is memory-hungry\n                    torch.profiler.ProfilerActivity.CUDA\n                ],\n                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n                on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=self.config.hardware.paths.logs.tensorboard),\n                profile_memory=True,\n                record_shapes=True,\n                with_stack=True,\n            )\n        return None\n\n    @cached_property\n    def loggers(self) -> List:\n        loggers = []\n        if self.config.diagnostics.log.wandb.enabled:\n            loggers.append(self.wandb_logger)\n        if self.config.diagnostics.log.tensorboard.enabled:\n            loggers.append(self.tensorboard_logger)\n        return loggers\n\n    @cached_property\n    def accelerator(self) -> str:\n        return \"gpu\" if self.config.hardware.num_gpus_per_node > 0 else \"cpu\"\n\n    def log_information(self) -> None:\n        # Log number of variables (features)\n        num_fc_features = self.config.data.num_features - self.config.data.num_aux_features\n        LOGGER.debug(\"Total number of prognostic variables: %d\", num_fc_features)\n        LOGGER.debug(\"Total number of auxiliary variables: %d\", self.config.data.num_aux_features)\n\n        # Log learning rate multiplier when running single-node, multi-GPU and/or multi-node\n        total_number_of_model_instances = (\n            self.config.hardware.num_nodes * self.config.hardware.num_gpus_per_node / self.config.hardware.num_gpus_per_model\n        )\n        LOGGER.debug(\n            \"Total GPU count / model group size: %d - NB: the learning rate will be scaled by this factor!\",\n            total_number_of_model_instances,\n        )\n        LOGGER.debug(\"Effective learning rate: %.3e\", total_number_of_model_instances * self.config.training.lr.rate)\n        LOGGER.debug(\"Rollout window length: %d\", self.config.training.rollout.start)\n\n    def update_paths(self) -> None:\n        \"\"\"Update the paths in the configuration.\"\"\"\n        self.config.hardware.paths.checkpoints = Path(self.config.hardware.paths.checkpoints, self.run_id)\n        self.config.hardware.paths.plots = Path(self.config.hardware.paths.plots, self.run_id)\n\n    def train(self) -> None:\n        \"\"\"Training entry point.\"\"\"\n\n        trainer = pl.Trainer(\n            accelerator=self.accelerator,\n            callbacks=self.callbacks,\n            deterministic=self.config.training.deterministic,\n            detect_anomaly=self.config.diagnostics.debug.anomaly_detection,\n            strategy=DDPGroupStrategy(self.config.hardware.num_gpus_per_model, static_graph=True),\n            devices=self.config.hardware.num_gpus_per_node,\n            num_nodes=self.config.hardware.num_nodes,\n            precision=self.config.training.precision,\n            max_epochs=self.config.training.max_epochs,\n            logger=self.loggers,\n            log_every_n_steps=self.config.diagnostics.log.interval,\n            # run a fixed no of batches per epoch (helpful when debugging)\n            limit_train_batches=self.config.dataloader.limit_batches.training,\n            limit_val_batches=self.config.dataloader.limit_batches.validation,\n            num_sanity_val_steps=0,\n            accumulate_grad_batches=self.config.training.accum_grad_batches,\n            gradient_clip_val=self.config.training.gradient_clip.val,\n            gradient_clip_algorithm=self.config.training.gradient_clip.algorithm,\n            # we have our own DDP-compliant sampler logic baked into the dataset\n            use_distributed_sampler=False,\n            profiler=self.profiler,\n        )\n\n        trainer.fit(self.model, datamodule=self.datamodule, ckpt_path=self.last_checkpoint)\n\n        LOGGER.debug(\"---- DONE. ----\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(config: DictConfig):\n    AIFSTrainer(config).train()\n"
    },
    {
      "file": "aifs/config/config.yaml",
      "description": "In our configs, we have split up keywords into different files.\n\nWe try to keep them as sensible values, but it's possible we have to override them sometimes. Especially for debugging or fancy new experiments.\n\nBelow you see an example of how you can change values in this main file.\n(But please don't commit those to your branch.)\n\nSo we can add\n\n```yaml\nhardware:\n    num_cpus_per_node: 1\n\n```\n\nto change the number of gpus that is visible to Pytorch to 1.",
      "line": 15,
      "title": "Adjusting the main configs",
      "contents": "defaults:\n  - hardware: atos\n  - data: zarr\n  - dataloader: default\n  - model: gnn\n  - training: default\n  - diagnostics: eval_rollout\n  - override hydra/job_logging: none\n  - override hydra/hydra_logging: none\n  - _self_\n\n    ### This file is for local experimentation.\nhardware:\n    num_cpus_per_node: 1\n##  When you commit your changes, assign the new features and keywords\n##  to the correct defaults.\n# For example to change from default GPU count:\n# hardware:\n#   num_gpus_per_node: 1\n"
    },
    {
      "file": "aifs/config/debug.yaml",
      "description": "Here's an example of our debug config that you saw earlier.\n\nWe use a smaller graph definition to speed up debugging, reduce the number of channels in the model and limit our batches to just 100.\n\nYou can familiarise yourself with different settings in the `aifs/config` folder, and later on use `Ctrl + Shift + F` to search the workspace for a setting you want to modify.",
      "line": 18,
      "title": "Visiting the debug config",
      "contents": "defaults:\n  - hardware: atos\n  - data: zarr\n  - dataloader: default\n  - model: gnn\n  - training: default\n  - diagnostics: eval_rollout\n  - override hydra/job_logging: none\n  - override hydra/hydra_logging: none\n  - _self_\n\n### This file is for local experimentation.\n##  When you commit your changes, assign the new features and keywords\n##  to the correct defaults.\n# For example to change from default GPU count:\n# hardware:\n#   num_gpus_per_node: 1\ndiagnostics:\n  log:\n    wandb:\n      offline: True\nhardware:\n  files:\n    graph: graph_mappings_normed_edge_attrs1_${data.resolution}_h_0_1_2_3_4.pt\nmodel:\n  num_channels: 128\ndataloader:\n  limit_batches:\n    training: 100\n    validation: 100\ntraining:\n  max_epochs: 3\n"
    },
    {
      "file": "aifs/train/train.py",
      "description": "Let's look at the `AIFSTrainer` class.\n\nThis is the main interface from the command line to our model.\n\nWe use Pytorch Lightning to train the model for its various convenience features.",
      "line": 24,
      "title": "The AIFS Trainer class",
      "contents": "from functools import cached_property\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport hydra\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.profilers import PyTorchProfiler\n\nfrom aifs.data.era_datamodule import ERA5DataModule\nfrom aifs.diagnostics.callbacks import get_callbacks\nfrom aifs.diagnostics.logging import get_tensorboard_logger\nfrom aifs.diagnostics.logging import get_wandb_logger\nfrom aifs.distributed.strategy import DDPGroupStrategy\nfrom aifs.train.forecaster import GraphForecaster\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass AIFSTrainer:\n    \"\"\"Utility class for training the model.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        # Set the default precision for all matmul operations to float32\n        torch.set_float32_matmul_precision(\"high\")\n        # Resolve the config to avoid shenanigans with lazy loading\n        OmegaConf.resolve(config)\n        self.config = config\n\n        # Default to not warm-starting from a checkpoint\n        self.start_from_checkpoint = bool(self.config.training.run_id) or bool(self.config.training.fork_run_id)\n        self.config.training.run_id = self.run_id\n\n        # Update paths to contain the run ID\n        self.update_paths()\n\n        self.log_information()\n\n    @cached_property\n    def datamodule(self) -> ERA5DataModule:\n        \"\"\"DataModule instance and DataSets.\"\"\"\n        return ERA5DataModule(self.config)\n\n    @cached_property\n    def initial_seed(self) -> int:\n        \"\"\"Initial seed for the RNG.\"\"\"\n        initial_seed = pl.seed_everything(self.config.training.initial_seed, workers=True)\n        LOGGER.debug(\"Running with initial seed: %d\", initial_seed)\n        return initial_seed\n\n    @cached_property\n    def model(self) -> GraphForecaster:\n        \"\"\"Provide the model instance.\"\"\"\n        return GraphForecaster(metadata=self.datamodule.input_metadata, config=self.config)\n\n    @cached_property\n    def run_id(self) -> str:\n        \"\"\"Unique identifier for the current run.\"\"\"\n\n        if self.config.training.run_id and not self.config.training.fork_run_id:\n            # Return the provided run ID\n            return self.config.training.run_id\n\n        # Generate a random UUID\n        import uuid\n\n        return str(uuid.uuid4())\n\n    @cached_property\n    def wandb_logger(self) -> pl.loggers.WandbLogger:\n        \"\"\"WandB logger.\"\"\"\n        return get_wandb_logger(self.config, self.model)\n\n    @cached_property\n    def tensorboard_logger(self) -> pl.loggers.TensorBoardLogger:\n        \"\"\"TensorBoard logger.\"\"\"\n        return get_tensorboard_logger(self.config)\n\n    @cached_property\n    def last_checkpoint(self) -> Optional[str]:\n        \"\"\"Path to the last checkpoint.\"\"\"\n        if not self.start_from_checkpoint:\n            return None\n\n        checkpoint = Path(\n            self.config.hardware.paths.checkpoints.parent,\n            self.config.training.fork_run_id or self.run_id,\n            self.config.hardware.files.warm_start or \"last.ckpt\",\n        )\n\n        # Check if the last checkpoint exists\n        if Path(checkpoint).exists():\n            LOGGER.info(\"Resuming training from last checkpoint: %s\", checkpoint)\n            return checkpoint\n        LOGGER.warning(\"Could not find last checkpoint: %s\", checkpoint)\n\n    @cached_property\n    def callbacks(self) -> List[pl.callbacks.Callback]:\n        return get_callbacks(self.config)\n\n    @cached_property\n    def profiler(self) -> Optional[PyTorchProfiler]:\n        \"\"\"Returns a pytorch profiler object, if profiling is enabled, otherwise\n        None.\"\"\"\n        if self.config.diagnostics.profiler:\n            assert (\n                self.config.diagnostics.log.tensorboard.enabled\n            ), \"Tensorboard logging must be enabled when profiling! Check your job config.\"\n            return PyTorchProfiler(\n                dirpath=self.config.hardware.paths.logs.tensorboard,\n                filename=\"aifs-profiler\",\n                export_to_chrome=False,\n                # profiler-specific keywords\n                activities=[\n                    # torch.profiler.ProfilerActivity.CPU,  # this is memory-hungry\n                    torch.profiler.ProfilerActivity.CUDA\n                ],\n                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n                on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=self.config.hardware.paths.logs.tensorboard),\n                profile_memory=True,\n                record_shapes=True,\n                with_stack=True,\n            )\n        return None\n\n    @cached_property\n    def loggers(self) -> List:\n        loggers = []\n        if self.config.diagnostics.log.wandb.enabled:\n            loggers.append(self.wandb_logger)\n        if self.config.diagnostics.log.tensorboard.enabled:\n            loggers.append(self.tensorboard_logger)\n        return loggers\n\n    @cached_property\n    def accelerator(self) -> str:\n        return \"gpu\" if self.config.hardware.num_gpus_per_node > 0 else \"cpu\"\n\n    def log_information(self) -> None:\n        # Log number of variables (features)\n        num_fc_features = self.config.data.num_features - self.config.data.num_aux_features\n        LOGGER.debug(\"Total number of prognostic variables: %d\", num_fc_features)\n        LOGGER.debug(\"Total number of auxiliary variables: %d\", self.config.data.num_aux_features)\n\n        # Log learning rate multiplier when running single-node, multi-GPU and/or multi-node\n        total_number_of_model_instances = (\n            self.config.hardware.num_nodes * self.config.hardware.num_gpus_per_node / self.config.hardware.num_gpus_per_model\n        )\n        LOGGER.debug(\n            \"Total GPU count / model group size: %d - NB: the learning rate will be scaled by this factor!\",\n            total_number_of_model_instances,\n        )\n        LOGGER.debug(\"Effective learning rate: %.3e\", total_number_of_model_instances * self.config.training.lr.rate)\n        LOGGER.debug(\"Rollout window length: %d\", self.config.training.rollout.start)\n\n    def update_paths(self) -> None:\n        \"\"\"Update the paths in the configuration.\"\"\"\n        self.config.hardware.paths.checkpoints = Path(self.config.hardware.paths.checkpoints, self.run_id)\n        self.config.hardware.paths.plots = Path(self.config.hardware.paths.plots, self.run_id)\n\n    def train(self) -> None:\n        \"\"\"Training entry point.\"\"\"\n\n        trainer = pl.Trainer(\n            accelerator=self.accelerator,\n            callbacks=self.callbacks,\n            deterministic=self.config.training.deterministic,\n            detect_anomaly=self.config.diagnostics.debug.anomaly_detection,\n            strategy=DDPGroupStrategy(self.config.hardware.num_gpus_per_model, static_graph=True),\n            devices=self.config.hardware.num_gpus_per_node,\n            num_nodes=self.config.hardware.num_nodes,\n            precision=self.config.training.precision,\n            max_epochs=self.config.training.max_epochs,\n            logger=self.loggers,\n            log_every_n_steps=self.config.diagnostics.log.interval,\n            # run a fixed no of batches per epoch (helpful when debugging)\n            limit_train_batches=self.config.dataloader.limit_batches.training,\n            limit_val_batches=self.config.dataloader.limit_batches.validation,\n            num_sanity_val_steps=0,\n            accumulate_grad_batches=self.config.training.accum_grad_batches,\n            gradient_clip_val=self.config.training.gradient_clip.val,\n            gradient_clip_algorithm=self.config.training.gradient_clip.algorithm,\n            # we have our own DDP-compliant sampler logic baked into the dataset\n            use_distributed_sampler=False,\n            profiler=self.profiler,\n        )\n\n        trainer.fit(self.model, datamodule=self.datamodule, ckpt_path=self.last_checkpoint)\n\n        LOGGER.debug(\"---- DONE. ----\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(config: DictConfig):\n    AIFSTrainer(config).train()\n"
    },
    {
      "file": "aifs/train/train.py",
      "description": "Here we deine the Pytorch Lightning Trainer.\n\nWe can pass in custom accelerators, callbacks, and settings that make our training more robust.",
      "line": 168,
      "title": "Defining the Trainer for Pytorch Lightning",
      "contents": "from functools import cached_property\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport hydra\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.profilers import PyTorchProfiler\n\nfrom aifs.data.era_datamodule import ERA5DataModule\nfrom aifs.diagnostics.callbacks import get_callbacks\nfrom aifs.diagnostics.logging import get_tensorboard_logger\nfrom aifs.diagnostics.logging import get_wandb_logger\nfrom aifs.distributed.strategy import DDPGroupStrategy\nfrom aifs.train.forecaster import GraphForecaster\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass AIFSTrainer:\n    \"\"\"Utility class for training the model.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        # Set the default precision for all matmul operations to float32\n        torch.set_float32_matmul_precision(\"high\")\n        # Resolve the config to avoid shenanigans with lazy loading\n        OmegaConf.resolve(config)\n        self.config = config\n\n        # Default to not warm-starting from a checkpoint\n        self.start_from_checkpoint = bool(self.config.training.run_id) or bool(self.config.training.fork_run_id)\n        self.config.training.run_id = self.run_id\n\n        # Update paths to contain the run ID\n        self.update_paths()\n\n        self.log_information()\n\n    @cached_property\n    def datamodule(self) -> ERA5DataModule:\n        \"\"\"DataModule instance and DataSets.\"\"\"\n        return ERA5DataModule(self.config)\n\n    @cached_property\n    def initial_seed(self) -> int:\n        \"\"\"Initial seed for the RNG.\"\"\"\n        initial_seed = pl.seed_everything(self.config.training.initial_seed, workers=True)\n        LOGGER.debug(\"Running with initial seed: %d\", initial_seed)\n        return initial_seed\n\n    @cached_property\n    def model(self) -> GraphForecaster:\n        \"\"\"Provide the model instance.\"\"\"\n        return GraphForecaster(metadata=self.datamodule.input_metadata, config=self.config)\n\n    @cached_property\n    def run_id(self) -> str:\n        \"\"\"Unique identifier for the current run.\"\"\"\n\n        if self.config.training.run_id and not self.config.training.fork_run_id:\n            # Return the provided run ID\n            return self.config.training.run_id\n\n        # Generate a random UUID\n        import uuid\n\n        return str(uuid.uuid4())\n\n    @cached_property\n    def wandb_logger(self) -> pl.loggers.WandbLogger:\n        \"\"\"WandB logger.\"\"\"\n        return get_wandb_logger(self.config, self.model)\n\n    @cached_property\n    def tensorboard_logger(self) -> pl.loggers.TensorBoardLogger:\n        \"\"\"TensorBoard logger.\"\"\"\n        return get_tensorboard_logger(self.config)\n\n    @cached_property\n    def last_checkpoint(self) -> Optional[str]:\n        \"\"\"Path to the last checkpoint.\"\"\"\n        if not self.start_from_checkpoint:\n            return None\n\n        checkpoint = Path(\n            self.config.hardware.paths.checkpoints.parent,\n            self.config.training.fork_run_id or self.run_id,\n            self.config.hardware.files.warm_start or \"last.ckpt\",\n        )\n\n        # Check if the last checkpoint exists\n        if Path(checkpoint).exists():\n            LOGGER.info(\"Resuming training from last checkpoint: %s\", checkpoint)\n            return checkpoint\n        LOGGER.warning(\"Could not find last checkpoint: %s\", checkpoint)\n\n    @cached_property\n    def callbacks(self) -> List[pl.callbacks.Callback]:\n        return get_callbacks(self.config)\n\n    @cached_property\n    def profiler(self) -> Optional[PyTorchProfiler]:\n        \"\"\"Returns a pytorch profiler object, if profiling is enabled, otherwise\n        None.\"\"\"\n        if self.config.diagnostics.profiler:\n            assert (\n                self.config.diagnostics.log.tensorboard.enabled\n            ), \"Tensorboard logging must be enabled when profiling! Check your job config.\"\n            return PyTorchProfiler(\n                dirpath=self.config.hardware.paths.logs.tensorboard,\n                filename=\"aifs-profiler\",\n                export_to_chrome=False,\n                # profiler-specific keywords\n                activities=[\n                    # torch.profiler.ProfilerActivity.CPU,  # this is memory-hungry\n                    torch.profiler.ProfilerActivity.CUDA\n                ],\n                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n                on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=self.config.hardware.paths.logs.tensorboard),\n                profile_memory=True,\n                record_shapes=True,\n                with_stack=True,\n            )\n        return None\n\n    @cached_property\n    def loggers(self) -> List:\n        loggers = []\n        if self.config.diagnostics.log.wandb.enabled:\n            loggers.append(self.wandb_logger)\n        if self.config.diagnostics.log.tensorboard.enabled:\n            loggers.append(self.tensorboard_logger)\n        return loggers\n\n    @cached_property\n    def accelerator(self) -> str:\n        return \"gpu\" if self.config.hardware.num_gpus_per_node > 0 else \"cpu\"\n\n    def log_information(self) -> None:\n        # Log number of variables (features)\n        num_fc_features = self.config.data.num_features - self.config.data.num_aux_features\n        LOGGER.debug(\"Total number of prognostic variables: %d\", num_fc_features)\n        LOGGER.debug(\"Total number of auxiliary variables: %d\", self.config.data.num_aux_features)\n\n        # Log learning rate multiplier when running single-node, multi-GPU and/or multi-node\n        total_number_of_model_instances = (\n            self.config.hardware.num_nodes * self.config.hardware.num_gpus_per_node / self.config.hardware.num_gpus_per_model\n        )\n        LOGGER.debug(\n            \"Total GPU count / model group size: %d - NB: the learning rate will be scaled by this factor!\",\n            total_number_of_model_instances,\n        )\n        LOGGER.debug(\"Effective learning rate: %.3e\", total_number_of_model_instances * self.config.training.lr.rate)\n        LOGGER.debug(\"Rollout window length: %d\", self.config.training.rollout.start)\n\n    def update_paths(self) -> None:\n        \"\"\"Update the paths in the configuration.\"\"\"\n        self.config.hardware.paths.checkpoints = Path(self.config.hardware.paths.checkpoints, self.run_id)\n        self.config.hardware.paths.plots = Path(self.config.hardware.paths.plots, self.run_id)\n\n    def train(self) -> None:\n        \"\"\"Training entry point.\"\"\"\n\n        trainer = pl.Trainer(\n            accelerator=self.accelerator,\n            callbacks=self.callbacks,\n            deterministic=self.config.training.deterministic,\n            detect_anomaly=self.config.diagnostics.debug.anomaly_detection,\n            strategy=DDPGroupStrategy(self.config.hardware.num_gpus_per_model, static_graph=True),\n            devices=self.config.hardware.num_gpus_per_node,\n            num_nodes=self.config.hardware.num_nodes,\n            precision=self.config.training.precision,\n            max_epochs=self.config.training.max_epochs,\n            logger=self.loggers,\n            log_every_n_steps=self.config.diagnostics.log.interval,\n            # run a fixed no of batches per epoch (helpful when debugging)\n            limit_train_batches=self.config.dataloader.limit_batches.training,\n            limit_val_batches=self.config.dataloader.limit_batches.validation,\n            num_sanity_val_steps=0,\n            accumulate_grad_batches=self.config.training.accum_grad_batches,\n            gradient_clip_val=self.config.training.gradient_clip.val,\n            gradient_clip_algorithm=self.config.training.gradient_clip.algorithm,\n            # we have our own DDP-compliant sampler logic baked into the dataset\n            use_distributed_sampler=False,\n            profiler=self.profiler,\n        )\n\n        trainer.fit(self.model, datamodule=self.datamodule, ckpt_path=self.last_checkpoint)\n\n        LOGGER.debug(\"---- DONE. ----\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(config: DictConfig):\n    AIFSTrainer(config).train()\n"
    },
    {
      "file": "aifs/train/train.py",
      "description": "Then we get the `fit()` interface that we know and love from Scikit-Learn et al.\n\nSo we pass the `model` and the `datamodule`, including an optional checkpoint for a warmstart.",
      "line": 192,
      "title": "Good ol' fit()",
      "contents": "from functools import cached_property\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport hydra\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.profilers import PyTorchProfiler\n\nfrom aifs.data.era_datamodule import ERA5DataModule\nfrom aifs.diagnostics.callbacks import get_callbacks\nfrom aifs.diagnostics.logging import get_tensorboard_logger\nfrom aifs.diagnostics.logging import get_wandb_logger\nfrom aifs.distributed.strategy import DDPGroupStrategy\nfrom aifs.train.forecaster import GraphForecaster\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass AIFSTrainer:\n    \"\"\"Utility class for training the model.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        # Set the default precision for all matmul operations to float32\n        torch.set_float32_matmul_precision(\"high\")\n        # Resolve the config to avoid shenanigans with lazy loading\n        OmegaConf.resolve(config)\n        self.config = config\n\n        # Default to not warm-starting from a checkpoint\n        self.start_from_checkpoint = bool(self.config.training.run_id) or bool(self.config.training.fork_run_id)\n        self.config.training.run_id = self.run_id\n\n        # Update paths to contain the run ID\n        self.update_paths()\n\n        self.log_information()\n\n    @cached_property\n    def datamodule(self) -> ERA5DataModule:\n        \"\"\"DataModule instance and DataSets.\"\"\"\n        return ERA5DataModule(self.config)\n\n    @cached_property\n    def initial_seed(self) -> int:\n        \"\"\"Initial seed for the RNG.\"\"\"\n        initial_seed = pl.seed_everything(self.config.training.initial_seed, workers=True)\n        LOGGER.debug(\"Running with initial seed: %d\", initial_seed)\n        return initial_seed\n\n    @cached_property\n    def model(self) -> GraphForecaster:\n        \"\"\"Provide the model instance.\"\"\"\n        return GraphForecaster(metadata=self.datamodule.input_metadata, config=self.config)\n\n    @cached_property\n    def run_id(self) -> str:\n        \"\"\"Unique identifier for the current run.\"\"\"\n\n        if self.config.training.run_id and not self.config.training.fork_run_id:\n            # Return the provided run ID\n            return self.config.training.run_id\n\n        # Generate a random UUID\n        import uuid\n\n        return str(uuid.uuid4())\n\n    @cached_property\n    def wandb_logger(self) -> pl.loggers.WandbLogger:\n        \"\"\"WandB logger.\"\"\"\n        return get_wandb_logger(self.config, self.model)\n\n    @cached_property\n    def tensorboard_logger(self) -> pl.loggers.TensorBoardLogger:\n        \"\"\"TensorBoard logger.\"\"\"\n        return get_tensorboard_logger(self.config)\n\n    @cached_property\n    def last_checkpoint(self) -> Optional[str]:\n        \"\"\"Path to the last checkpoint.\"\"\"\n        if not self.start_from_checkpoint:\n            return None\n\n        checkpoint = Path(\n            self.config.hardware.paths.checkpoints.parent,\n            self.config.training.fork_run_id or self.run_id,\n            self.config.hardware.files.warm_start or \"last.ckpt\",\n        )\n\n        # Check if the last checkpoint exists\n        if Path(checkpoint).exists():\n            LOGGER.info(\"Resuming training from last checkpoint: %s\", checkpoint)\n            return checkpoint\n        LOGGER.warning(\"Could not find last checkpoint: %s\", checkpoint)\n\n    @cached_property\n    def callbacks(self) -> List[pl.callbacks.Callback]:\n        return get_callbacks(self.config)\n\n    @cached_property\n    def profiler(self) -> Optional[PyTorchProfiler]:\n        \"\"\"Returns a pytorch profiler object, if profiling is enabled, otherwise\n        None.\"\"\"\n        if self.config.diagnostics.profiler:\n            assert (\n                self.config.diagnostics.log.tensorboard.enabled\n            ), \"Tensorboard logging must be enabled when profiling! Check your job config.\"\n            return PyTorchProfiler(\n                dirpath=self.config.hardware.paths.logs.tensorboard,\n                filename=\"aifs-profiler\",\n                export_to_chrome=False,\n                # profiler-specific keywords\n                activities=[\n                    # torch.profiler.ProfilerActivity.CPU,  # this is memory-hungry\n                    torch.profiler.ProfilerActivity.CUDA\n                ],\n                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n                on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=self.config.hardware.paths.logs.tensorboard),\n                profile_memory=True,\n                record_shapes=True,\n                with_stack=True,\n            )\n        return None\n\n    @cached_property\n    def loggers(self) -> List:\n        loggers = []\n        if self.config.diagnostics.log.wandb.enabled:\n            loggers.append(self.wandb_logger)\n        if self.config.diagnostics.log.tensorboard.enabled:\n            loggers.append(self.tensorboard_logger)\n        return loggers\n\n    @cached_property\n    def accelerator(self) -> str:\n        return \"gpu\" if self.config.hardware.num_gpus_per_node > 0 else \"cpu\"\n\n    def log_information(self) -> None:\n        # Log number of variables (features)\n        num_fc_features = self.config.data.num_features - self.config.data.num_aux_features\n        LOGGER.debug(\"Total number of prognostic variables: %d\", num_fc_features)\n        LOGGER.debug(\"Total number of auxiliary variables: %d\", self.config.data.num_aux_features)\n\n        # Log learning rate multiplier when running single-node, multi-GPU and/or multi-node\n        total_number_of_model_instances = (\n            self.config.hardware.num_nodes * self.config.hardware.num_gpus_per_node / self.config.hardware.num_gpus_per_model\n        )\n        LOGGER.debug(\n            \"Total GPU count / model group size: %d - NB: the learning rate will be scaled by this factor!\",\n            total_number_of_model_instances,\n        )\n        LOGGER.debug(\"Effective learning rate: %.3e\", total_number_of_model_instances * self.config.training.lr.rate)\n        LOGGER.debug(\"Rollout window length: %d\", self.config.training.rollout.start)\n\n    def update_paths(self) -> None:\n        \"\"\"Update the paths in the configuration.\"\"\"\n        self.config.hardware.paths.checkpoints = Path(self.config.hardware.paths.checkpoints, self.run_id)\n        self.config.hardware.paths.plots = Path(self.config.hardware.paths.plots, self.run_id)\n\n    def train(self) -> None:\n        \"\"\"Training entry point.\"\"\"\n\n        trainer = pl.Trainer(\n            accelerator=self.accelerator,\n            callbacks=self.callbacks,\n            deterministic=self.config.training.deterministic,\n            detect_anomaly=self.config.diagnostics.debug.anomaly_detection,\n            strategy=DDPGroupStrategy(self.config.hardware.num_gpus_per_model, static_graph=True),\n            devices=self.config.hardware.num_gpus_per_node,\n            num_nodes=self.config.hardware.num_nodes,\n            precision=self.config.training.precision,\n            max_epochs=self.config.training.max_epochs,\n            logger=self.loggers,\n            log_every_n_steps=self.config.diagnostics.log.interval,\n            # run a fixed no of batches per epoch (helpful when debugging)\n            limit_train_batches=self.config.dataloader.limit_batches.training,\n            limit_val_batches=self.config.dataloader.limit_batches.validation,\n            num_sanity_val_steps=0,\n            accumulate_grad_batches=self.config.training.accum_grad_batches,\n            gradient_clip_val=self.config.training.gradient_clip.val,\n            gradient_clip_algorithm=self.config.training.gradient_clip.algorithm,\n            # we have our own DDP-compliant sampler logic baked into the dataset\n            use_distributed_sampler=False,\n            profiler=self.profiler,\n        )\n\n        trainer.fit(self.model, datamodule=self.datamodule, ckpt_path=self.last_checkpoint)\n\n        LOGGER.debug(\"---- DONE. ----\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(config: DictConfig):\n    AIFSTrainer(config).train()\n"
    },
    {
      "file": "aifs/train/train.py",
      "description": "The datamodule lives over here and simply taps into our `ecml-tools` under the hood where we load the training data zarr files.\n\nBut we can just change this in [`aifs/config/dataloader`](https://github.com/ecmwf-lab/aifs-mono/blob/master/aifs/config/dataloader/default.yaml) and never touch the actual code to switch out our data!",
      "line": 44,
      "title": "Where does the Data come from?",
      "contents": "from functools import cached_property\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport hydra\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.profilers import PyTorchProfiler\n\nfrom aifs.data.era_datamodule import ERA5DataModule\nfrom aifs.diagnostics.callbacks import get_callbacks\nfrom aifs.diagnostics.logging import get_tensorboard_logger\nfrom aifs.diagnostics.logging import get_wandb_logger\nfrom aifs.distributed.strategy import DDPGroupStrategy\nfrom aifs.train.forecaster import GraphForecaster\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass AIFSTrainer:\n    \"\"\"Utility class for training the model.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        # Set the default precision for all matmul operations to float32\n        torch.set_float32_matmul_precision(\"high\")\n        # Resolve the config to avoid shenanigans with lazy loading\n        OmegaConf.resolve(config)\n        self.config = config\n\n        # Default to not warm-starting from a checkpoint\n        self.start_from_checkpoint = bool(self.config.training.run_id) or bool(self.config.training.fork_run_id)\n        self.config.training.run_id = self.run_id\n\n        # Update paths to contain the run ID\n        self.update_paths()\n\n        self.log_information()\n\n    @cached_property\n    def datamodule(self) -> ERA5DataModule:\n        \"\"\"DataModule instance and DataSets.\"\"\"\n        return ERA5DataModule(self.config)\n\n    @cached_property\n    def initial_seed(self) -> int:\n        \"\"\"Initial seed for the RNG.\"\"\"\n        initial_seed = pl.seed_everything(self.config.training.initial_seed, workers=True)\n        LOGGER.debug(\"Running with initial seed: %d\", initial_seed)\n        return initial_seed\n\n    @cached_property\n    def model(self) -> GraphForecaster:\n        \"\"\"Provide the model instance.\"\"\"\n        return GraphForecaster(metadata=self.datamodule.input_metadata, config=self.config)\n\n    @cached_property\n    def run_id(self) -> str:\n        \"\"\"Unique identifier for the current run.\"\"\"\n\n        if self.config.training.run_id and not self.config.training.fork_run_id:\n            # Return the provided run ID\n            return self.config.training.run_id\n\n        # Generate a random UUID\n        import uuid\n\n        return str(uuid.uuid4())\n\n    @cached_property\n    def wandb_logger(self) -> pl.loggers.WandbLogger:\n        \"\"\"WandB logger.\"\"\"\n        return get_wandb_logger(self.config, self.model)\n\n    @cached_property\n    def tensorboard_logger(self) -> pl.loggers.TensorBoardLogger:\n        \"\"\"TensorBoard logger.\"\"\"\n        return get_tensorboard_logger(self.config)\n\n    @cached_property\n    def last_checkpoint(self) -> Optional[str]:\n        \"\"\"Path to the last checkpoint.\"\"\"\n        if not self.start_from_checkpoint:\n            return None\n\n        checkpoint = Path(\n            self.config.hardware.paths.checkpoints.parent,\n            self.config.training.fork_run_id or self.run_id,\n            self.config.hardware.files.warm_start or \"last.ckpt\",\n        )\n\n        # Check if the last checkpoint exists\n        if Path(checkpoint).exists():\n            LOGGER.info(\"Resuming training from last checkpoint: %s\", checkpoint)\n            return checkpoint\n        LOGGER.warning(\"Could not find last checkpoint: %s\", checkpoint)\n\n    @cached_property\n    def callbacks(self) -> List[pl.callbacks.Callback]:\n        return get_callbacks(self.config)\n\n    @cached_property\n    def profiler(self) -> Optional[PyTorchProfiler]:\n        \"\"\"Returns a pytorch profiler object, if profiling is enabled, otherwise\n        None.\"\"\"\n        if self.config.diagnostics.profiler:\n            assert (\n                self.config.diagnostics.log.tensorboard.enabled\n            ), \"Tensorboard logging must be enabled when profiling! Check your job config.\"\n            return PyTorchProfiler(\n                dirpath=self.config.hardware.paths.logs.tensorboard,\n                filename=\"aifs-profiler\",\n                export_to_chrome=False,\n                # profiler-specific keywords\n                activities=[\n                    # torch.profiler.ProfilerActivity.CPU,  # this is memory-hungry\n                    torch.profiler.ProfilerActivity.CUDA\n                ],\n                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n                on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=self.config.hardware.paths.logs.tensorboard),\n                profile_memory=True,\n                record_shapes=True,\n                with_stack=True,\n            )\n        return None\n\n    @cached_property\n    def loggers(self) -> List:\n        loggers = []\n        if self.config.diagnostics.log.wandb.enabled:\n            loggers.append(self.wandb_logger)\n        if self.config.diagnostics.log.tensorboard.enabled:\n            loggers.append(self.tensorboard_logger)\n        return loggers\n\n    @cached_property\n    def accelerator(self) -> str:\n        return \"gpu\" if self.config.hardware.num_gpus_per_node > 0 else \"cpu\"\n\n    def log_information(self) -> None:\n        # Log number of variables (features)\n        num_fc_features = self.config.data.num_features - self.config.data.num_aux_features\n        LOGGER.debug(\"Total number of prognostic variables: %d\", num_fc_features)\n        LOGGER.debug(\"Total number of auxiliary variables: %d\", self.config.data.num_aux_features)\n\n        # Log learning rate multiplier when running single-node, multi-GPU and/or multi-node\n        total_number_of_model_instances = (\n            self.config.hardware.num_nodes * self.config.hardware.num_gpus_per_node / self.config.hardware.num_gpus_per_model\n        )\n        LOGGER.debug(\n            \"Total GPU count / model group size: %d - NB: the learning rate will be scaled by this factor!\",\n            total_number_of_model_instances,\n        )\n        LOGGER.debug(\"Effective learning rate: %.3e\", total_number_of_model_instances * self.config.training.lr.rate)\n        LOGGER.debug(\"Rollout window length: %d\", self.config.training.rollout.start)\n\n    def update_paths(self) -> None:\n        \"\"\"Update the paths in the configuration.\"\"\"\n        self.config.hardware.paths.checkpoints = Path(self.config.hardware.paths.checkpoints, self.run_id)\n        self.config.hardware.paths.plots = Path(self.config.hardware.paths.plots, self.run_id)\n\n    def train(self) -> None:\n        \"\"\"Training entry point.\"\"\"\n\n        trainer = pl.Trainer(\n            accelerator=self.accelerator,\n            callbacks=self.callbacks,\n            deterministic=self.config.training.deterministic,\n            detect_anomaly=self.config.diagnostics.debug.anomaly_detection,\n            strategy=DDPGroupStrategy(self.config.hardware.num_gpus_per_model, static_graph=True),\n            devices=self.config.hardware.num_gpus_per_node,\n            num_nodes=self.config.hardware.num_nodes,\n            precision=self.config.training.precision,\n            max_epochs=self.config.training.max_epochs,\n            logger=self.loggers,\n            log_every_n_steps=self.config.diagnostics.log.interval,\n            # run a fixed no of batches per epoch (helpful when debugging)\n            limit_train_batches=self.config.dataloader.limit_batches.training,\n            limit_val_batches=self.config.dataloader.limit_batches.validation,\n            num_sanity_val_steps=0,\n            accumulate_grad_batches=self.config.training.accum_grad_batches,\n            gradient_clip_val=self.config.training.gradient_clip.val,\n            gradient_clip_algorithm=self.config.training.gradient_clip.algorithm,\n            # we have our own DDP-compliant sampler logic baked into the dataset\n            use_distributed_sampler=False,\n            profiler=self.profiler,\n        )\n\n        trainer.fit(self.model, datamodule=self.datamodule, ckpt_path=self.last_checkpoint)\n\n        LOGGER.debug(\"---- DONE. ----\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(config: DictConfig):\n    AIFSTrainer(config).train()\n"
    },
    {
      "file": "aifs/train/train.py",
      "description": "You can see we try to use `@cached_property` as much as possible to make values available in the class.",
      "line": 43,
      "title": "Cached_properties are really nice",
      "contents": "from functools import cached_property\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport hydra\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.profilers import PyTorchProfiler\n\nfrom aifs.data.era_datamodule import ERA5DataModule\nfrom aifs.diagnostics.callbacks import get_callbacks\nfrom aifs.diagnostics.logging import get_tensorboard_logger\nfrom aifs.diagnostics.logging import get_wandb_logger\nfrom aifs.distributed.strategy import DDPGroupStrategy\nfrom aifs.train.forecaster import GraphForecaster\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass AIFSTrainer:\n    \"\"\"Utility class for training the model.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        # Set the default precision for all matmul operations to float32\n        torch.set_float32_matmul_precision(\"high\")\n        # Resolve the config to avoid shenanigans with lazy loading\n        OmegaConf.resolve(config)\n        self.config = config\n\n        # Default to not warm-starting from a checkpoint\n        self.start_from_checkpoint = bool(self.config.training.run_id) or bool(self.config.training.fork_run_id)\n        self.config.training.run_id = self.run_id\n\n        # Update paths to contain the run ID\n        self.update_paths()\n\n        self.log_information()\n\n    @cached_property\n    def datamodule(self) -> ERA5DataModule:\n        \"\"\"DataModule instance and DataSets.\"\"\"\n        return ERA5DataModule(self.config)\n\n    @cached_property\n    def initial_seed(self) -> int:\n        \"\"\"Initial seed for the RNG.\"\"\"\n        initial_seed = pl.seed_everything(self.config.training.initial_seed, workers=True)\n        LOGGER.debug(\"Running with initial seed: %d\", initial_seed)\n        return initial_seed\n\n    @cached_property\n    def model(self) -> GraphForecaster:\n        \"\"\"Provide the model instance.\"\"\"\n        return GraphForecaster(metadata=self.datamodule.input_metadata, config=self.config)\n\n    @cached_property\n    def run_id(self) -> str:\n        \"\"\"Unique identifier for the current run.\"\"\"\n\n        if self.config.training.run_id and not self.config.training.fork_run_id:\n            # Return the provided run ID\n            return self.config.training.run_id\n\n        # Generate a random UUID\n        import uuid\n\n        return str(uuid.uuid4())\n\n    @cached_property\n    def wandb_logger(self) -> pl.loggers.WandbLogger:\n        \"\"\"WandB logger.\"\"\"\n        return get_wandb_logger(self.config, self.model)\n\n    @cached_property\n    def tensorboard_logger(self) -> pl.loggers.TensorBoardLogger:\n        \"\"\"TensorBoard logger.\"\"\"\n        return get_tensorboard_logger(self.config)\n\n    @cached_property\n    def last_checkpoint(self) -> Optional[str]:\n        \"\"\"Path to the last checkpoint.\"\"\"\n        if not self.start_from_checkpoint:\n            return None\n\n        checkpoint = Path(\n            self.config.hardware.paths.checkpoints.parent,\n            self.config.training.fork_run_id or self.run_id,\n            self.config.hardware.files.warm_start or \"last.ckpt\",\n        )\n\n        # Check if the last checkpoint exists\n        if Path(checkpoint).exists():\n            LOGGER.info(\"Resuming training from last checkpoint: %s\", checkpoint)\n            return checkpoint\n        LOGGER.warning(\"Could not find last checkpoint: %s\", checkpoint)\n\n    @cached_property\n    def callbacks(self) -> List[pl.callbacks.Callback]:\n        return get_callbacks(self.config)\n\n    @cached_property\n    def profiler(self) -> Optional[PyTorchProfiler]:\n        \"\"\"Returns a pytorch profiler object, if profiling is enabled, otherwise\n        None.\"\"\"\n        if self.config.diagnostics.profiler:\n            assert (\n                self.config.diagnostics.log.tensorboard.enabled\n            ), \"Tensorboard logging must be enabled when profiling! Check your job config.\"\n            return PyTorchProfiler(\n                dirpath=self.config.hardware.paths.logs.tensorboard,\n                filename=\"aifs-profiler\",\n                export_to_chrome=False,\n                # profiler-specific keywords\n                activities=[\n                    # torch.profiler.ProfilerActivity.CPU,  # this is memory-hungry\n                    torch.profiler.ProfilerActivity.CUDA\n                ],\n                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n                on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=self.config.hardware.paths.logs.tensorboard),\n                profile_memory=True,\n                record_shapes=True,\n                with_stack=True,\n            )\n        return None\n\n    @cached_property\n    def loggers(self) -> List:\n        loggers = []\n        if self.config.diagnostics.log.wandb.enabled:\n            loggers.append(self.wandb_logger)\n        if self.config.diagnostics.log.tensorboard.enabled:\n            loggers.append(self.tensorboard_logger)\n        return loggers\n\n    @cached_property\n    def accelerator(self) -> str:\n        return \"gpu\" if self.config.hardware.num_gpus_per_node > 0 else \"cpu\"\n\n    def log_information(self) -> None:\n        # Log number of variables (features)\n        num_fc_features = self.config.data.num_features - self.config.data.num_aux_features\n        LOGGER.debug(\"Total number of prognostic variables: %d\", num_fc_features)\n        LOGGER.debug(\"Total number of auxiliary variables: %d\", self.config.data.num_aux_features)\n\n        # Log learning rate multiplier when running single-node, multi-GPU and/or multi-node\n        total_number_of_model_instances = (\n            self.config.hardware.num_nodes * self.config.hardware.num_gpus_per_node / self.config.hardware.num_gpus_per_model\n        )\n        LOGGER.debug(\n            \"Total GPU count / model group size: %d - NB: the learning rate will be scaled by this factor!\",\n            total_number_of_model_instances,\n        )\n        LOGGER.debug(\"Effective learning rate: %.3e\", total_number_of_model_instances * self.config.training.lr.rate)\n        LOGGER.debug(\"Rollout window length: %d\", self.config.training.rollout.start)\n\n    def update_paths(self) -> None:\n        \"\"\"Update the paths in the configuration.\"\"\"\n        self.config.hardware.paths.checkpoints = Path(self.config.hardware.paths.checkpoints, self.run_id)\n        self.config.hardware.paths.plots = Path(self.config.hardware.paths.plots, self.run_id)\n\n    def train(self) -> None:\n        \"\"\"Training entry point.\"\"\"\n\n        trainer = pl.Trainer(\n            accelerator=self.accelerator,\n            callbacks=self.callbacks,\n            deterministic=self.config.training.deterministic,\n            detect_anomaly=self.config.diagnostics.debug.anomaly_detection,\n            strategy=DDPGroupStrategy(self.config.hardware.num_gpus_per_model, static_graph=True),\n            devices=self.config.hardware.num_gpus_per_node,\n            num_nodes=self.config.hardware.num_nodes,\n            precision=self.config.training.precision,\n            max_epochs=self.config.training.max_epochs,\n            logger=self.loggers,\n            log_every_n_steps=self.config.diagnostics.log.interval,\n            # run a fixed no of batches per epoch (helpful when debugging)\n            limit_train_batches=self.config.dataloader.limit_batches.training,\n            limit_val_batches=self.config.dataloader.limit_batches.validation,\n            num_sanity_val_steps=0,\n            accumulate_grad_batches=self.config.training.accum_grad_batches,\n            gradient_clip_val=self.config.training.gradient_clip.val,\n            gradient_clip_algorithm=self.config.training.gradient_clip.algorithm,\n            # we have our own DDP-compliant sampler logic baked into the dataset\n            use_distributed_sampler=False,\n            profiler=self.profiler,\n        )\n\n        trainer.fit(self.model, datamodule=self.datamodule, ckpt_path=self.last_checkpoint)\n\n        LOGGER.debug(\"---- DONE. ----\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(config: DictConfig):\n    AIFSTrainer(config).train()\n"
    },
    {
      "file": "aifs/train/train.py",
      "description": "And here we get our model, the `GraphForecaster`!",
      "line": 56,
      "title": "Where does the Model live?",
      "contents": "from functools import cached_property\nfrom pathlib import Path\nfrom typing import List\nfrom typing import Optional\n\nimport hydra\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom pytorch_lightning.profilers import PyTorchProfiler\n\nfrom aifs.data.era_datamodule import ERA5DataModule\nfrom aifs.diagnostics.callbacks import get_callbacks\nfrom aifs.diagnostics.logging import get_tensorboard_logger\nfrom aifs.diagnostics.logging import get_wandb_logger\nfrom aifs.distributed.strategy import DDPGroupStrategy\nfrom aifs.train.forecaster import GraphForecaster\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass AIFSTrainer:\n    \"\"\"Utility class for training the model.\"\"\"\n\n    def __init__(self, config: DictConfig):\n        # Set the default precision for all matmul operations to float32\n        torch.set_float32_matmul_precision(\"high\")\n        # Resolve the config to avoid shenanigans with lazy loading\n        OmegaConf.resolve(config)\n        self.config = config\n\n        # Default to not warm-starting from a checkpoint\n        self.start_from_checkpoint = bool(self.config.training.run_id) or bool(self.config.training.fork_run_id)\n        self.config.training.run_id = self.run_id\n\n        # Update paths to contain the run ID\n        self.update_paths()\n\n        self.log_information()\n\n    @cached_property\n    def datamodule(self) -> ERA5DataModule:\n        \"\"\"DataModule instance and DataSets.\"\"\"\n        return ERA5DataModule(self.config)\n\n    @cached_property\n    def initial_seed(self) -> int:\n        \"\"\"Initial seed for the RNG.\"\"\"\n        initial_seed = pl.seed_everything(self.config.training.initial_seed, workers=True)\n        LOGGER.debug(\"Running with initial seed: %d\", initial_seed)\n        return initial_seed\n\n    @cached_property\n    def model(self) -> GraphForecaster:\n        \"\"\"Provide the model instance.\"\"\"\n        return GraphForecaster(metadata=self.datamodule.input_metadata, config=self.config)\n\n    @cached_property\n    def run_id(self) -> str:\n        \"\"\"Unique identifier for the current run.\"\"\"\n\n        if self.config.training.run_id and not self.config.training.fork_run_id:\n            # Return the provided run ID\n            return self.config.training.run_id\n\n        # Generate a random UUID\n        import uuid\n\n        return str(uuid.uuid4())\n\n    @cached_property\n    def wandb_logger(self) -> pl.loggers.WandbLogger:\n        \"\"\"WandB logger.\"\"\"\n        return get_wandb_logger(self.config, self.model)\n\n    @cached_property\n    def tensorboard_logger(self) -> pl.loggers.TensorBoardLogger:\n        \"\"\"TensorBoard logger.\"\"\"\n        return get_tensorboard_logger(self.config)\n\n    @cached_property\n    def last_checkpoint(self) -> Optional[str]:\n        \"\"\"Path to the last checkpoint.\"\"\"\n        if not self.start_from_checkpoint:\n            return None\n\n        checkpoint = Path(\n            self.config.hardware.paths.checkpoints.parent,\n            self.config.training.fork_run_id or self.run_id,\n            self.config.hardware.files.warm_start or \"last.ckpt\",\n        )\n\n        # Check if the last checkpoint exists\n        if Path(checkpoint).exists():\n            LOGGER.info(\"Resuming training from last checkpoint: %s\", checkpoint)\n            return checkpoint\n        LOGGER.warning(\"Could not find last checkpoint: %s\", checkpoint)\n\n    @cached_property\n    def callbacks(self) -> List[pl.callbacks.Callback]:\n        return get_callbacks(self.config)\n\n    @cached_property\n    def profiler(self) -> Optional[PyTorchProfiler]:\n        \"\"\"Returns a pytorch profiler object, if profiling is enabled, otherwise\n        None.\"\"\"\n        if self.config.diagnostics.profiler:\n            assert (\n                self.config.diagnostics.log.tensorboard.enabled\n            ), \"Tensorboard logging must be enabled when profiling! Check your job config.\"\n            return PyTorchProfiler(\n                dirpath=self.config.hardware.paths.logs.tensorboard,\n                filename=\"aifs-profiler\",\n                export_to_chrome=False,\n                # profiler-specific keywords\n                activities=[\n                    # torch.profiler.ProfilerActivity.CPU,  # this is memory-hungry\n                    torch.profiler.ProfilerActivity.CUDA\n                ],\n                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n                on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=self.config.hardware.paths.logs.tensorboard),\n                profile_memory=True,\n                record_shapes=True,\n                with_stack=True,\n            )\n        return None\n\n    @cached_property\n    def loggers(self) -> List:\n        loggers = []\n        if self.config.diagnostics.log.wandb.enabled:\n            loggers.append(self.wandb_logger)\n        if self.config.diagnostics.log.tensorboard.enabled:\n            loggers.append(self.tensorboard_logger)\n        return loggers\n\n    @cached_property\n    def accelerator(self) -> str:\n        return \"gpu\" if self.config.hardware.num_gpus_per_node > 0 else \"cpu\"\n\n    def log_information(self) -> None:\n        # Log number of variables (features)\n        num_fc_features = self.config.data.num_features - self.config.data.num_aux_features\n        LOGGER.debug(\"Total number of prognostic variables: %d\", num_fc_features)\n        LOGGER.debug(\"Total number of auxiliary variables: %d\", self.config.data.num_aux_features)\n\n        # Log learning rate multiplier when running single-node, multi-GPU and/or multi-node\n        total_number_of_model_instances = (\n            self.config.hardware.num_nodes * self.config.hardware.num_gpus_per_node / self.config.hardware.num_gpus_per_model\n        )\n        LOGGER.debug(\n            \"Total GPU count / model group size: %d - NB: the learning rate will be scaled by this factor!\",\n            total_number_of_model_instances,\n        )\n        LOGGER.debug(\"Effective learning rate: %.3e\", total_number_of_model_instances * self.config.training.lr.rate)\n        LOGGER.debug(\"Rollout window length: %d\", self.config.training.rollout.start)\n\n    def update_paths(self) -> None:\n        \"\"\"Update the paths in the configuration.\"\"\"\n        self.config.hardware.paths.checkpoints = Path(self.config.hardware.paths.checkpoints, self.run_id)\n        self.config.hardware.paths.plots = Path(self.config.hardware.paths.plots, self.run_id)\n\n    def train(self) -> None:\n        \"\"\"Training entry point.\"\"\"\n\n        trainer = pl.Trainer(\n            accelerator=self.accelerator,\n            callbacks=self.callbacks,\n            deterministic=self.config.training.deterministic,\n            detect_anomaly=self.config.diagnostics.debug.anomaly_detection,\n            strategy=DDPGroupStrategy(self.config.hardware.num_gpus_per_model, static_graph=True),\n            devices=self.config.hardware.num_gpus_per_node,\n            num_nodes=self.config.hardware.num_nodes,\n            precision=self.config.training.precision,\n            max_epochs=self.config.training.max_epochs,\n            logger=self.loggers,\n            log_every_n_steps=self.config.diagnostics.log.interval,\n            # run a fixed no of batches per epoch (helpful when debugging)\n            limit_train_batches=self.config.dataloader.limit_batches.training,\n            limit_val_batches=self.config.dataloader.limit_batches.validation,\n            num_sanity_val_steps=0,\n            accumulate_grad_batches=self.config.training.accum_grad_batches,\n            gradient_clip_val=self.config.training.gradient_clip.val,\n            gradient_clip_algorithm=self.config.training.gradient_clip.algorithm,\n            # we have our own DDP-compliant sampler logic baked into the dataset\n            use_distributed_sampler=False,\n            profiler=self.profiler,\n        )\n\n        trainer.fit(self.model, datamodule=self.datamodule, ckpt_path=self.last_checkpoint)\n\n        LOGGER.debug(\"---- DONE. ----\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(config: DictConfig):\n    AIFSTrainer(config).train()\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "The `GraphForecaster` is the pytorch lightning module, where we define the training behaviour, losses, and everything needed for the actual training-validation-test loop.",
      "line": 28,
      "title": "Pytorch Lightning model code",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "Our graph is saved externally as a file on disk, which we load and cache here.\n\nThis defines the \"connectivity\" of our nodes.",
      "line": 49,
      "title": "The Graph",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "Under the Pytorch Lightning hood, we drop down to a pure Pytorch model.\n\nWhy?\n\nWell, this means we can define stand-alone Inference Checkpoints that only need a minimal subset of dependencies!",
      "line": 51,
      "title": "The hidden Pytorch model",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "Then we define how our losses are scaled.",
      "line": 64,
      "title": "Scaling your losses",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "And which loss function to use.\n\nIn this case, we're using the weighted mean squared error, but ensemble training would switch this out for a probabilistic metric.",
      "line": 82,
      "title": "Defining our Loss function",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "Our forward step is very simple. We just run the data through our model.",
      "line": 124,
      "title": "forward()",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "A training step consists of our abstracted `_step` method and some logging.",
      "line": 176,
      "title": "The Training steps",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "The `_step()` method is the heart of the training code in this Pytorch lightning module.\n\nHere we define our input data, normalise the data, and define a way to roll our predictions out over multiple timesteps.",
      "line": 139,
      "title": "The common _step() infrastructure",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "We use multiple steps as inputs that we extract from the batch.",
      "line": 149,
      "title": "Starting the rollout training",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "Then roll out predictions over the whole rollout range we define in the configs.",
      "line": 153,
      "title": "Going through the rollout step loop",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "And then we pass the multistep data into the Lightning module, which in turn calls the `forward()` method from [before](#15).",
      "line": 155,
      "title": "Predicting the next weather state",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "And `advance_input()` rolls our new data in place for the next multistep input in our rollout.",
      "line": 161,
      "title": "Rolling our input data",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    },
    {
      "file": "aifs/train/forecaster.py",
      "description": "Now we just scale the loss according to the number of rollout steps and return our loss and metrics, including the predictions that we use in the validation plots.",
      "line": 173,
      "title": "Scaling the loss to the rollout",
      "contents": "import math\nimport os\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Mapping\nfrom typing import Tuple\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom omegaconf import DictConfig\nfrom omegaconf import OmegaConf\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.utils.checkpoint import checkpoint\n\nfrom aifs.data.scaling import pressure_level\nfrom aifs.model.losses import grad_scaler\nfrom aifs.model.losses import WeightedMSELoss\nfrom aifs.model.model import AIFSModelGNN\nfrom aifs.utils.config import DotConfig\nfrom aifs.utils.logger import get_code_logger\n\nLOGGER = get_code_logger(__name__)\n\n\nclass GraphForecaster(pl.LightningModule):\n    \"\"\"Graph neural network forecaster for PyTorch Lightning.\"\"\"\n\n    def __init__(\n        self,\n        metadata: Dict,\n        config: DictConfig,\n    ) -> None:\n        \"\"\"Initialize graph neural network forecaster.\n\n        Parameters\n        ----------\n        metadata : Dict\n            Zarr metadata\n        config : DictConfig\n            Job configuration\n        \"\"\"\n        super().__init__()\n\n        self.fcdim = config.data.num_features - config.data.num_aux_features\n        num_levels = len(config.data.pl.levels)\n\n        self.graph_data = torch.load(Path(config.hardware.paths.graph, config.hardware.files.graph))\n\n        self.model = AIFSModelGNN(\n            metadata=metadata,\n            graph_data=self.graph_data,\n            config=DotConfig(OmegaConf.to_container(config, resolve=True)),\n        )\n\n        self.save_hyperparameters()\n\n        self.era_latlons = self.graph_data[(\"era\", \"to\", \"era\")].ecoords_rad\n        self.era_weights = self.graph_data[(\"era\", \"to\", \"era\")].area_weights\n\n        self.logger_enabled = config.diagnostics.log.wandb.enabled\n\n        loss_scaling = np.array([], dtype=np.float32)\n        for pl_name in config.data.pl.parameters:\n            if pl_name in config.training.loss_scaling.pl:\n                scl = config.training.loss_scaling.pl[pl_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", pl_name)\n            loss_scaling = np.append(loss_scaling, [scl] * pressure_level(config.data.pl.levels))\n        for sfc_name in config.data.sfc.parameters:\n            if sfc_name in config.training.loss_scaling.sfc:\n                scl = config.training.loss_scaling.sfc[sfc_name]\n            else:\n                scl = 1\n                LOGGER.debug(\"Parameter %s was not scaled.\", sfc_name)\n            loss_scaling = np.append(loss_scaling, [scl])\n        assert len(loss_scaling) == self.fcdim\n        loss_scaling = torch.from_numpy(loss_scaling)\n\n        self.loss = WeightedMSELoss(area_weights=self.era_weights, data_variances=loss_scaling)\n        if config.training.loss_gradient_scaling:\n            self.loss.register_full_backward_hook(grad_scaler, prepend=False)\n\n        self.metric_ranges = {}\n        for i, key in enumerate(config.data.pl.parameters):\n            self.metric_ranges[key] = [i * num_levels, (i + 1) * num_levels]\n        for key in config.training.metrics:\n            idx = metadata[\"name_to_index\"][key]\n            self.metric_ranges[key] = [idx, idx + 1]\n        self.metrics = WeightedMSELoss(area_weights=self.era_weights)\n\n        self.multi_step = config.training.multistep_input\n        self.lr = (\n            config.hardware.num_nodes\n            * config.hardware.num_gpus_per_node\n            * config.training.lr.rate\n            / config.hardware.num_gpus_per_model\n        )\n        self.lr_iterations = config.training.lr.iterations\n        self.lr_min = config.training.lr.min\n        self.rollout = config.training.rollout.start\n        self.rollout_epoch_increment = config.training.rollout.epoch_increment\n        self.rollout_max = config.training.rollout.max\n\n        self.use_zero_optimizer = config.training.zero_optimizer\n\n        self.model_comm_group = None\n\n        LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        LOGGER.debug(\"Rollout increase every : %d epochs\", self.rollout_epoch_increment)\n        LOGGER.debug(\"Rollout max : %d\", self.rollout_max)\n        LOGGER.debug(\"Multistep: %d\", self.multi_step)\n\n        self.enable_plot = config.diagnostics.plot.enabled\n\n        self.model_comm_group_id = int(os.environ.get(\"SLURM_PROCID\", \"0\")) // config.hardware.num_gpus_per_model\n        self.model_comm_group_rank = int(os.environ.get(\"SLURM_PROCID\", \"0\")) % config.hardware.num_gpus_per_model\n        self.model_comm_num_groups = math.ceil(\n            config.hardware.num_gpus_per_node * config.hardware.num_nodes / config.hardware.num_gpus_per_model\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x, self.model_comm_group)\n\n    def set_model_comm_group(self, model_comm_group) -> None:\n        LOGGER.debug(\"set_model_comm_group: %s\", model_comm_group)\n        self.model_comm_group = model_comm_group\n\n    def advance_input(self, x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        x = x.roll(-1, dims=1)\n        # autoregressive predictions - we re-init the \"variable\" part of x\n        x[:, self.multi_step - 1, :, : self.fcdim] = y_pred\n        # get new \"constants\" needed for time-varying fields\n        x[:, self.multi_step - 1, :, self.fcdim :] = y[..., self.fcdim :]\n        return x\n\n    def _step(\n        self,\n        batch: torch.Tensor,\n        batch_idx: int,\n        validation_mode: bool = False,\n    ) -> Tuple[torch.Tensor, Mapping[str, torch.Tensor]]:\n        loss = torch.zeros(1, dtype=batch.dtype, device=self.device, requires_grad=False)\n        batch = self.model.normalizer(batch)  # normalized in-place\n        metrics = {}\n\n        # start rollout\n        x = batch[:, 0 : self.multi_step, ...]  # (bs, multi_step, latlon, nvar)\n\n        y_preds = []\n        for rstep in range(self.rollout):\n            # if rstep > 0: torch.cuda.empty_cache() # uncomment if rollout fails with OOM\n            y_pred = self(x)  # prediction at rollout step rstep, shape = (bs, latlon, nvar)\n\n            y = batch[:, self.multi_step + rstep, ...]  # target, shape = (bs, latlon, nvar)\n            # y includes the auxiliary variables, so we must leave those out when computing the loss\n            loss += checkpoint(self.loss, y_pred, y[..., : self.fcdim], use_reentrant=False)\n\n            x = self.advance_input(x, y, y_pred)\n\n            if validation_mode:\n                for mkey, (low, high) in self.metric_ranges.items():\n                    y_denorm = self.model.normalizer.denormalize(y, in_place=False)\n                    y_hat_denorm = self.model.normalizer.denormalize(x[:, -1, ...], in_place=False)\n                    metrics[f\"{mkey}_{rstep+1}\"] = self.metrics(y_hat_denorm[..., low:high], y_denorm[..., low:high])\n\n                if self.enable_plot:\n                    y_preds.append(y_pred)\n\n        # scale loss\n        loss *= 1.0 / self.rollout\n        return loss, metrics, y_preds\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n        train_loss, _, _ = self._step(batch, batch_idx)\n        self.log(\n            \"train_wmse\",\n            train_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        self.log(\n            \"rollout\",\n            float(self.rollout),\n            on_step=True,\n            logger=self.logger_enabled,\n            rank_zero_only=True,\n            sync_dist=False,\n        )\n        return train_loss\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.trainer.global_step)\n\n    def on_train_epoch_end(self):\n        if self.rollout_epoch_increment > 0 and self.current_epoch % self.rollout_epoch_increment == 0:\n            self.rollout += 1\n            LOGGER.debug(\"Rollout window length: %d\", self.rollout)\n        self.rollout = min(self.rollout, self.rollout_max)\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> None:\n        with torch.no_grad():\n            val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n        self.log(\n            \"val_wmse\",\n            val_loss,\n            on_epoch=True,\n            on_step=True,\n            prog_bar=True,\n            logger=self.logger_enabled,\n            batch_size=batch.shape[0],\n            sync_dist=True,\n        )\n        for mname, mvalue in metrics.items():\n            self.log(\n                \"val_\" + mname,\n                mvalue,\n                on_epoch=True,\n                on_step=False,\n                prog_bar=False,\n                logger=self.logger_enabled,\n                batch_size=batch.shape[0],\n                sync_dist=True,\n            )\n        return val_loss, y_preds\n\n    def predict_step(self, batch: torch.Tensor) -> torch.Tensor:\n        batch = self.normalizer(batch, in_place=False)\n\n        with torch.no_grad():\n            x = batch[:, 0 : self.multi_step, ...]\n            y_hat = self(x)\n\n        return self.normalizer.denormalize(y_hat, in_place=False)\n\n    def configure_optimizers(self):\n        if self.use_zero_optimizer:\n            optimizer = ZeroRedundancyOptimizer(\n                self.trainer.model.parameters(), optimizer_class=torch.optim.AdamW, betas=(0.9, 0.95), lr=self.lr\n            )\n        else:\n            optimizer = torch.optim.AdamW(self.trainer.model.parameters(), betas=(0.9, 0.95), lr=self.lr)  # , fused=True)\n\n        scheduler = CosineLRScheduler(\n            optimizer,\n            lr_min=self.lr_min,\n            t_initial=self.lr_iterations,\n            warmup_t=1000,\n        )\n        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n"
    }
  ],
  "description": "Getting to know the basics of AIFS training"
}
